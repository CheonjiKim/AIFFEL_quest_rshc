{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84e133e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 1. 필요한 라이브러리 불러오기\n",
    "# =============================================================================\n",
    "from tensorflow.keras.datasets import reuters\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "\n",
    "# 벡터화 함수\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# 머신러닝 모델들\n",
    "from sklearn.naive_bayes import MultinomialNB #다항분포 나이브 베이즈 모델\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# 모델 검증\n",
    "from sklearn.metrics import accuracy_score #정확도 계산\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7b4d90",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================\n",
      "[실험] num_words = 10000\n",
      "  문장의 최대 길이: 2376, 평균 길이: 145.54\n",
      "\n",
      "[1] Multinomial Naive Bayes\n",
      "  정확도: 0.6567230632235085\n",
      "  F1-Score: 0.5764467518778252\n",
      "\n",
      "[2] Complement Naive Bayes\n",
      "  정확도: 0.7707034728406055\n",
      "  F1-Score: 0.7456682614453047\n",
      "\n",
      "[3] Logistic Regression (L2, random_state=42)\n",
      "  정확도: 0.7951914514692787\n",
      "  F1-Score: 0.7727935299669956\n",
      "\n",
      "[4] Support Vector Machine (LinearSVC, random_state=42)\n",
      "  정확도: 0.8299198575244879\n",
      "  F1-Score: 0.8236882254849623\n",
      "\n",
      "[5] Decision Tree (max_depth=10, random_state=42)\n",
      "  정확도: 0.6219946571682992\n",
      "  F1-Score: 0.5787746084604964\n",
      "\n",
      "[6] Random Forest (n_estimators=5, random_state=42)\n",
      "  정확도: 0.6776491540516474\n",
      "  F1-Score: 0.6448193857300015\n",
      "\n",
      "[7] Gradient Boosting (n_estimators=100, random_state=42)\n",
      "  정확도: 0.7707034728406055\n",
      "  F1-Score: 0.7664689841505945\n",
      "\n",
      "[8] Voting Classifier (Soft Voting)\n",
      "  정확도: 0.7983081032947462\n",
      "  F1-Score: 0.7935713406786632\n",
      "===================================================\n",
      "[실험] num_words = 15000\n",
      "  문장의 최대 길이: 2376, 평균 길이: 145.54\n",
      "\n",
      "[1] Multinomial Naive Bayes\n",
      "  정확도: 0.6331255565449688\n",
      "  F1-Score: 0.5498212868794679\n",
      "\n",
      "[2] Complement Naive Bayes\n",
      "  정확도: 0.7720391807658059\n",
      "  F1-Score: 0.7448186439256785\n",
      "\n",
      "[3] Logistic Regression (L2, random_state=42)\n",
      "  정확도: 0.792520035618878\n",
      "  F1-Score: 0.7681513627651299\n",
      "\n",
      "[4] Support Vector Machine (LinearSVC, random_state=42)\n",
      "  정확도: 0.8281389136242209\n",
      "  F1-Score: 0.8220497065470049\n",
      "\n",
      "[5] Decision Tree (max_depth=10, random_state=42)\n",
      "  정확도: 0.6184327693677649\n",
      "  F1-Score: 0.5742227860548131\n",
      "\n",
      "[6] Random Forest (n_estimators=5, random_state=42)\n",
      "  정확도: 0.674087266251113\n",
      "  F1-Score: 0.6479366611682628\n",
      "\n",
      "[7] Gradient Boosting (n_estimators=100, random_state=42)\n",
      "  정확도: 0.7662511130899377\n",
      "  F1-Score: 0.763251744728771\n",
      "\n",
      "[8] Voting Classifier (Soft Voting)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 반복할 num_words 옵션\n",
    "num_words_options = [10000, 15000]\n",
    "ml_results = {}  # 전체 결과를 저장할 딕셔너리\n",
    "\n",
    "for nw in num_words_options:\n",
    "    print(\"===================================================\")\n",
    "    print(f\"[실험] num_words = {nw}\")\n",
    "    \n",
    "    # 1) 데이터 로드\n",
    "    (x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=nw, test_split=0.2)\n",
    "\n",
    "    # 2) 단어 사전 로드 및 인덱스→단어 매핑\n",
    "    word_index = reuters.get_word_index(path=\"reuters_word_index.json\")\n",
    "    index_to_word = {index + 3: word for word, index in word_index.items()}\n",
    "    for index, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\n",
    "        index_to_word[index] = token\n",
    "\n",
    "    # 3) 정수 시퀀스를 텍스트로 복원\n",
    "    x_train_text = [' '.join([index_to_word.get(idx, \"<unk>\") for idx in seq]) for seq in x_train]\n",
    "    x_test_text  = [' '.join([index_to_word.get(idx, \"<unk>\") for idx in seq]) for seq in x_test]\n",
    "\n",
    "    # 문장 길이 통계\n",
    "    lengths = [len(txt.split()) for txt in x_train_text]\n",
    "    max_length = np.max(lengths)\n",
    "    avg_length = np.mean(lengths)\n",
    "    print(f\"  문장의 최대 길이: {max_length}, 평균 길이: {avg_length:.2f}\")\n",
    "\n",
    "    # 4) TF-IDF 전처리\n",
    "    dtmvector = CountVectorizer()\n",
    "    x_train_dtm = dtmvector.fit_transform(x_train_text)\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    tfidfv = tfidf_transformer.fit_transform(x_train_dtm)\n",
    "\n",
    "    x_test_dtm = dtmvector.transform(x_test_text)\n",
    "    tfidfv_test = tfidf_transformer.transform(x_test_dtm)\n",
    "\n",
    "    # 5) 모델 학습 및 평가 (Accuracy, F1-score)\n",
    "    #    모델별 결과(f1-score)를 저장할 딕셔너리\n",
    "    experiment_result = {}\n",
    "\n",
    "    # (a) Multinomial Naive Bayes\n",
    "    print(\"\\n[1] Multinomial Naive Bayes\")\n",
    "    model_mnb = MultinomialNB()\n",
    "    model_mnb.fit(tfidfv, y_train)\n",
    "    predicted_mnb = model_mnb.predict(tfidfv_test)\n",
    "    acc_mnb = accuracy_score(y_test, predicted_mnb)\n",
    "    f1_mnb = f1_score(y_test, predicted_mnb, average='weighted')\n",
    "    print(\"  정확도:\", acc_mnb)\n",
    "    print(\"  F1-Score:\", f1_mnb)\n",
    "    experiment_result['MultinomialNB'] = f1_mnb\n",
    "\n",
    "    # (b) Complement Naive Bayes\n",
    "    print(\"\\n[2] Complement Naive Bayes\")\n",
    "    model_cnb = ComplementNB()\n",
    "    model_cnb.fit(tfidfv, y_train)\n",
    "    predicted_cnb = model_cnb.predict(tfidfv_test)\n",
    "    acc_cnb = accuracy_score(y_test, predicted_cnb)\n",
    "    f1_cnb = f1_score(y_test, predicted_cnb, average='weighted')\n",
    "    print(\"  정확도:\", acc_cnb)\n",
    "    print(\"  F1-Score:\", f1_cnb)\n",
    "    experiment_result['ComplementNB'] = f1_cnb\n",
    "\n",
    "    # (c) Logistic Regression\n",
    "    print(\"\\n[3] Logistic Regression \")\n",
    "    model_logreg = LogisticRegression(C=10000, penalty='l2', max_iter=1000, random_state=42)\n",
    "    model_logreg.fit(tfidfv, y_train)\n",
    "    predicted_logreg = model_logreg.predict(tfidfv_test)\n",
    "    acc_logreg = accuracy_score(y_test, predicted_logreg)\n",
    "    f1_logreg = f1_score(y_test, predicted_logreg, average='weighted')\n",
    "    print(\"  정확도:\", acc_logreg)\n",
    "    print(\"  F1-Score:\", f1_logreg)\n",
    "    experiment_result['LogisticRegression'] = f1_logreg\n",
    "\n",
    "    # (d) LinearSVC (\n",
    "    print(\"\\n[4] Support Vector Machine \")\n",
    "    model_svc = LinearSVC(C=1000, penalty='l1', max_iter=500, dual=False, random_state=42)\n",
    "    model_svc.fit(tfidfv, y_train)\n",
    "    predicted_svc = model_svc.predict(tfidfv_test)\n",
    "    acc_svc = accuracy_score(y_test, predicted_svc)\n",
    "    f1_svc = f1_score(y_test, predicted_svc, average='weighted')\n",
    "    print(\"  정확도:\", acc_svc)\n",
    "    print(\"  F1-Score:\", f1_svc)\n",
    "    experiment_result['LinearSVC'] = f1_svc\n",
    "\n",
    "    # (e) Decision Tree\n",
    "    print(\"\\n[5] Decision Tree \")\n",
    "    model_tree = DecisionTreeClassifier(max_depth=10, random_state=42)\n",
    "    model_tree.fit(tfidfv, y_train)\n",
    "    predicted_tree = model_tree.predict(tfidfv_test)\n",
    "    acc_tree = accuracy_score(y_test, predicted_tree)\n",
    "    f1_tree = f1_score(y_test, predicted_tree, average='weighted')\n",
    "    print(\"  정확도:\", acc_tree)\n",
    "    print(\"  F1-Score:\", f1_tree)\n",
    "    experiment_result['DecisionTree'] = f1_tree\n",
    "\n",
    "    # (f) Random Forest\n",
    "    print(\"\\n[6] Random Forest\")\n",
    "    model_forest = RandomForestClassifier(n_estimators=5, random_state=42)\n",
    "    model_forest.fit(tfidfv, y_train)\n",
    "    predicted_forest = model_forest.predict(tfidfv_test)\n",
    "    acc_forest = accuracy_score(y_test, predicted_forest)\n",
    "    f1_forest = f1_score(y_test, predicted_forest, average='weighted')\n",
    "    print(\"  정확도:\", acc_forest)\n",
    "    print(\"  F1-Score:\", f1_forest)\n",
    "    experiment_result['RandomForest'] = f1_forest\n",
    "\n",
    "    # (g) Gradient Boosting\n",
    "    print(\"\\n[7] Gradient Boosting\")\n",
    "    model_gbt = GradientBoostingClassifier(random_state=0, verbose=0)\n",
    "    model_gbt.fit(tfidfv, y_train)\n",
    "    predicted_gbt = model_gbt.predict(tfidfv_test)\n",
    "    acc_gbt = accuracy_score(y_test, predicted_gbt)\n",
    "    f1_gbt = f1_score(y_test, predicted_gbt, average='weighted')\n",
    "    print(\"  정확도:\", acc_gbt)\n",
    "    print(\"  F1-Score:\", f1_gbt)\n",
    "    experiment_result['GradientBoosting'] = f1_gbt\n",
    "\n",
    "    # (h) Voting Classifier\n",
    "    print(\"\\n[8] Voting Classifier \")\n",
    "    voting_classifier = VotingClassifier(\n",
    "        estimators=[\n",
    "            ('lr', LogisticRegression(C=10000, penalty='l2')),\n",
    "            ('cb', ComplementNB()),\n",
    "            ('grbt', GradientBoostingClassifier(random_state=0))\n",
    "        ],\n",
    "        voting='soft'\n",
    "    )\n",
    "    voting_classifier.fit(tfidfv, y_train)\n",
    "    predicted_vote = voting_classifier.predict(tfidfv_test)\n",
    "    acc_vote = accuracy_score(y_test, predicted_vote)\n",
    "    f1_vote = f1_score(y_test, predicted_vote, average='weighted')\n",
    "    print(\"  정확도:\", acc_vote)\n",
    "    print(\"  F1-Score:\", f1_vote)\n",
    "    experiment_result['Voting'] = f1_vote\n",
    "\n",
    "    # 최종 결과 저장\n",
    "    ml_results[str(nw)] = experiment_result\n",
    "\n",
    "# 6) 전체 결과 요약\n",
    "print(\"\\n===================================================\")\n",
    "print(\"전체 머신러닝 실험 결과 (weighted f1-score):\")\n",
    "for nw, res_dict in ml_results.items():\n",
    "    print(f\"\\n>>> num_words = {nw}\")\n",
    "    for model_name, f1_val in res_dict.items():\n",
    "        print(f\"  {model_name}: {f1_val:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38e5ca22",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================\n",
      "실험: num_words = 10000, 모델 = Conv1D\n",
      "문장의 최대 길이: 2376\n",
      "Epoch 1/5\n",
      "280/280 [==============================] - 3s 11ms/step - loss: 1.8117 - accuracy: 0.5545\n",
      "Epoch 2/5\n",
      "280/280 [==============================] - 3s 10ms/step - loss: 1.2071 - accuracy: 0.7022\n",
      "Epoch 3/5\n",
      "280/280 [==============================] - 3s 11ms/step - loss: 0.8417 - accuracy: 0.7849\n",
      "Epoch 4/5\n",
      "280/280 [==============================] - 3s 11ms/step - loss: 0.5669 - accuracy: 0.8554\n",
      "Epoch 5/5\n",
      "280/280 [==============================] - 3s 10ms/step - loss: 0.3934 - accuracy: 0.9035\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 1.6365 - accuracy: 0.6960\n",
      "Test Loss: 1.6365\n",
      "Test Accuracy: 0.6960\n",
      "정확도: 0.6955\n",
      "F1-Score: 0.6760\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.33      0.47        12\n",
      "           1       0.59      0.71      0.65       105\n",
      "           2       0.18      0.10      0.13        20\n",
      "           3       0.90      0.90      0.90       813\n",
      "           4       0.73      0.88      0.80       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.67      0.57      0.62        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.66      0.55      0.60        38\n",
      "           9       0.24      0.56      0.34        25\n",
      "          10       0.38      0.17      0.23        30\n",
      "          11       0.59      0.53      0.56        83\n",
      "          12       0.43      0.23      0.30        13\n",
      "          13       0.37      0.35      0.36        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.56      0.75      0.64        99\n",
      "          17       0.50      0.08      0.14        12\n",
      "          18       0.62      0.50      0.56        20\n",
      "          19       0.58      0.60      0.59       133\n",
      "          20       0.33      0.33      0.33        70\n",
      "          21       0.15      0.15      0.15        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.17      0.08      0.11        12\n",
      "          24       0.27      0.16      0.20        19\n",
      "          25       0.90      0.29      0.44        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.00      0.00      0.00        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.14      0.15      0.15        13\n",
      "          32       0.33      0.30      0.32        10\n",
      "          33       0.00      0.00      0.00         5\n",
      "          34       0.33      0.14      0.20         7\n",
      "          35       0.00      0.00      0.00         6\n",
      "          36       0.08      0.09      0.08        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.33      0.10      0.15        10\n",
      "          41       1.00      0.12      0.22         8\n",
      "          42       0.25      0.33      0.29         3\n",
      "          43       0.50      0.33      0.40         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.70      2246\n",
      "   macro avg       0.34      0.25      0.27      2246\n",
      "weighted avg       0.68      0.70      0.68      2246\n",
      "\n",
      "==============================================\n",
      "실험: num_words = 10000, 모델 = RNN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장의 최대 길이: 2376\n",
      "Epoch 1/5\n",
      "280/280 [==============================] - 20s 63ms/step - loss: 2.2942 - accuracy: 0.4184\n",
      "Epoch 2/5\n",
      "280/280 [==============================] - 18s 63ms/step - loss: 1.9368 - accuracy: 0.5015\n",
      "Epoch 3/5\n",
      "280/280 [==============================] - 18s 63ms/step - loss: 1.6580 - accuracy: 0.5731\n",
      "Epoch 4/5\n",
      "280/280 [==============================] - 18s 63ms/step - loss: 1.5003 - accuracy: 0.6166\n",
      "Epoch 5/5\n",
      "280/280 [==============================] - 18s 63ms/step - loss: 1.4004 - accuracy: 0.6394\n",
      "70/70 [==============================] - 3s 29ms/step - loss: 1.5689 - accuracy: 0.6036\n",
      "Test Loss: 1.5689\n",
      "Test Accuracy: 0.6036\n",
      "정확도: 0.6046\n",
      "F1-Score: 0.5664\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        12\n",
      "           1       0.34      0.58      0.43       105\n",
      "           2       0.00      0.00      0.00        20\n",
      "           3       0.90      0.95      0.92       813\n",
      "           4       0.89      0.83      0.86       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.00      0.00      0.00        14\n",
      "           7       0.00      0.00      0.00         3\n",
      "           8       0.00      0.00      0.00        38\n",
      "           9       0.00      0.00      0.00        25\n",
      "          10       0.00      0.00      0.00        30\n",
      "          11       0.06      0.07      0.07        83\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       0.00      0.00      0.00        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.10      0.20      0.14        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.18      0.15      0.16        20\n",
      "          19       0.22      0.77      0.35       133\n",
      "          20       0.00      0.00      0.00        70\n",
      "          21       0.00      0.00      0.00        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       0.00      0.00      0.00        19\n",
      "          25       0.00      0.00      0.00        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.00      0.00      0.00        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       0.00      0.00      0.00        10\n",
      "          33       0.00      0.00      0.00         5\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       0.00      0.00      0.00         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.00      0.00      0.00         6\n",
      "          44       0.00      0.00      0.00         5\n",
      "          45       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.60      2246\n",
      "   macro avg       0.06      0.08      0.06      2246\n",
      "weighted avg       0.55      0.60      0.57      2246\n",
      "\n",
      "==============================================\n",
      "실험: num_words = 15000, 모델 = Conv1D\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장의 최대 길이: 2376\n",
      "Epoch 1/5\n",
      "280/280 [==============================] - 3s 11ms/step - loss: 1.8328 - accuracy: 0.5385\n",
      "Epoch 2/5\n",
      "280/280 [==============================] - 3s 11ms/step - loss: 1.1723 - accuracy: 0.7119\n",
      "Epoch 3/5\n",
      "280/280 [==============================] - 3s 11ms/step - loss: 0.7848 - accuracy: 0.7954\n",
      "Epoch 4/5\n",
      "280/280 [==============================] - 3s 11ms/step - loss: 0.5245 - accuracy: 0.8734\n",
      "Epoch 5/5\n",
      "280/280 [==============================] - 3s 11ms/step - loss: 0.3449 - accuracy: 0.9136\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 1.5788 - accuracy: 0.7103\n",
      "Test Loss: 1.5788\n",
      "Test Accuracy: 0.7103\n",
      "정확도: 0.7097\n",
      "F1-Score: 0.6939\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.33      0.44        12\n",
      "           1       0.60      0.72      0.66       105\n",
      "           2       0.26      0.35      0.30        20\n",
      "           3       0.90      0.92      0.91       813\n",
      "           4       0.78      0.88      0.83       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.79      0.79      0.79        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.61      0.58      0.59        38\n",
      "           9       0.71      0.40      0.51        25\n",
      "          10       0.15      0.20      0.17        30\n",
      "          11       0.49      0.64      0.55        83\n",
      "          12       0.43      0.23      0.30        13\n",
      "          13       0.31      0.43      0.36        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.52      0.66      0.58        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.30      0.35      0.33        20\n",
      "          19       0.67      0.59      0.62       133\n",
      "          20       0.56      0.43      0.48        70\n",
      "          21       0.24      0.26      0.25        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.14      0.08      0.11        12\n",
      "          24       0.31      0.21      0.25        19\n",
      "          25       0.92      0.35      0.51        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.33      0.10      0.15        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.40      0.17      0.24        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       0.20      0.10      0.13        10\n",
      "          33       0.00      0.00      0.00         5\n",
      "          34       0.50      0.14      0.22         7\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       0.25      0.09      0.13        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.50      0.20      0.29         5\n",
      "          40       1.00      0.10      0.18        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.20      0.17      0.18         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.71      2246\n",
      "   macro avg       0.39      0.28      0.30      2246\n",
      "weighted avg       0.70      0.71      0.69      2246\n",
      "\n",
      "==============================================\n",
      "실험: num_words = 15000, 모델 = RNN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장의 최대 길이: 2376\n",
      "Epoch 1/5\n",
      "280/280 [==============================] - 20s 64ms/step - loss: 2.3422 - accuracy: 0.4140\n",
      "Epoch 2/5\n",
      "280/280 [==============================] - 18s 63ms/step - loss: 1.7970 - accuracy: 0.5408\n",
      "Epoch 3/5\n",
      "280/280 [==============================] - 18s 63ms/step - loss: 1.6032 - accuracy: 0.5766\n",
      "Epoch 4/5\n",
      "280/280 [==============================] - 18s 63ms/step - loss: 1.4687 - accuracy: 0.6015\n",
      "Epoch 5/5\n",
      "280/280 [==============================] - 18s 63ms/step - loss: 1.3244 - accuracy: 0.6395\n",
      "70/70 [==============================] - 2s 27ms/step - loss: 1.6040 - accuracy: 0.6125\n",
      "Test Loss: 1.6040\n",
      "Test Accuracy: 0.6125\n",
      "정확도: 0.6126\n",
      "F1-Score: 0.5860\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        12\n",
      "           1       0.20      0.56      0.30       105\n",
      "           2       0.00      0.00      0.00        20\n",
      "           3       0.91      0.93      0.92       813\n",
      "           4       0.77      0.87      0.82       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.00      0.00      0.00        14\n",
      "           7       0.00      0.00      0.00         3\n",
      "           8       0.00      0.00      0.00        38\n",
      "           9       0.11      0.44      0.17        25\n",
      "          10       0.09      0.37      0.14        30\n",
      "          11       0.17      0.25      0.20        83\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       0.00      0.00      0.00        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.51      0.18      0.27        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.28      0.25      0.26        20\n",
      "          19       0.53      0.50      0.51       133\n",
      "          20       0.48      0.19      0.27        70\n",
      "          21       0.22      0.07      0.11        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       0.00      0.00      0.00        19\n",
      "          25       0.05      0.03      0.04        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.00      0.00      0.00        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       0.00      0.00      0.00        10\n",
      "          33       0.00      0.00      0.00         5\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       0.00      0.00      0.00         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.00      0.00      0.00         6\n",
      "          44       0.00      0.00      0.00         5\n",
      "          45       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.61      2246\n",
      "   macro avg       0.09      0.10      0.09      2246\n",
      "weighted avg       0.59      0.61      0.59      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import reuters\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "\n",
    "def tensorflow_dataset(input_x, input_y, buffer_size, batch_size):\n",
    "    tf_data = tf.data.Dataset.from_tensor_slices((input_x, input_y))\n",
    "    tf_data = tf_data.shuffle(buffer_size)\n",
    "    tf_data = tf_data.repeat()\n",
    "    tf_data = tf_data.batch(batch_size)\n",
    "    tf_data = tf_data.prefetch(buffer_size=-1)\n",
    "    return tf_data \n",
    "\n",
    "\n",
    "# 2. Conv1D 모델 (Baseline)\n",
    "class Conv1D(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedd_size, num_classes):\n",
    "        super(Conv1D, self).__init__()\n",
    "        self.embedd = tf.keras.layers.Embedding(input_dim=vocab_size,\n",
    "                                                output_dim=embedd_size)\n",
    "        self.cnn = tf.keras.layers.Conv1D(64, kernel_size=3, activation='relu')\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dense = tf.keras.layers.Dense(64, activation=\"relu\")\n",
    "        self.outputs = tf.keras.layers.Dense(num_classes, activation=\"softmax\")\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.embedd(x)\n",
    "        x = self.cnn(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense(x)\n",
    "        x = self.outputs(x)\n",
    "        return x\n",
    "\n",
    "# 2-2. RNN 모델 (LSTM 기반)\n",
    "class RNNModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedd_size, num_classes):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(input_dim=vocab_size,\n",
    "                                                   output_dim=embedd_size)\n",
    "        self.lstm = tf.keras.layers.LSTM(64)\n",
    "        self.dense = tf.keras.layers.Dense(64, activation='relu')\n",
    "        self.out = tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.lstm(x)\n",
    "        x = self.dense(x)\n",
    "        return self.out(x)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 3. 실험 설정: num_words와 모델 종류에 따른 4가지 경우\n",
    "# ---------------------------------------------------------------------------\n",
    "experiments = [\n",
    "    {\"num_words\": 10000, \"model_type\": \"Conv1D\"},\n",
    "    {\"num_words\": 10000, \"model_type\": \"RNN\"},\n",
    "    {\"num_words\": 15000, \"model_type\": \"Conv1D\"},\n",
    "    {\"num_words\": 15000, \"model_type\": \"RNN\"}\n",
    "]\n",
    "\n",
    "batch_size = 32\n",
    "embedd_size = 100\n",
    "epochs = 5\n",
    "\n",
    "for exp in experiments:\n",
    "    num_words_param = exp[\"num_words\"]\n",
    "    model_type = exp[\"model_type\"]\n",
    "    \n",
    "    print(\"==============================================\")\n",
    "    print(f\"실험: num_words = {num_words_param}, 모델 = {model_type}\")\n",
    "    \n",
    "    # 데이터 로드 (Reuters 데이터는 num_words 인자에 따라 상위 단어만 사용)\n",
    "    (x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=num_words_param, test_split=0.2)\n",
    "    \n",
    "    # 훈련 데이터의 최대 길이 계산\n",
    "    max_len = np.max(list(map(lambda x: len(x), x_train)))\n",
    "    print(f\"문장의 최대 길이: {max_len}\")\n",
    "    \n",
    "    # 훈련셋, 테스트셋 패딩\n",
    "    pad_x_train = pad_sequences(x_train, maxlen=max_len)\n",
    "    pad_x_test = pad_sequences(x_test, maxlen=max_len)\n",
    "    \n",
    "    # y 데이터 reshape (텐서플로우 데이터셋 생성을 위해)\n",
    "    y_train_reshaped = np.array(y_train).reshape(-1, 1)\n",
    "    y_test_reshaped = np.array(y_test).reshape(-1, 1)\n",
    "    \n",
    "    buffer_size = len(y_train_reshaped)\n",
    "    \n",
    "    tf_train = tensorflow_dataset(pad_x_train, y_train_reshaped, buffer_size, batch_size)\n",
    "    total_num_test = len(pad_x_test)\n",
    "    tf_test = tensorflow_dataset(pad_x_test, y_test_reshaped, buffer_size, batch_size)\n",
    "    \n",
    "    vocab_size = num_words_param  \n",
    "    num_classes = np.max(y_train) + 1\n",
    "    \n",
    "    # 모델 선택: Conv1D 또는 RNN\n",
    "    if model_type == \"Conv1D\":\n",
    "        model = Conv1D(vocab_size, embedd_size, num_classes)\n",
    "    elif model_type == \"RNN\":\n",
    "        model = RNNModel(vocab_size, embedd_size, num_classes)\n",
    "    else:\n",
    "        print(\"모델 타입 오류\")\n",
    "        continue\n",
    "    \n",
    "    # 모델 컴파일 및 학습\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                  optimizer=\"adam\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "    \n",
    "    model.fit(tf_train,\n",
    "              steps_per_epoch=buffer_size // batch_size,\n",
    "              epochs=epochs)\n",
    "    \n",
    "    # 모델 평가\n",
    "    test_loss, test_acc = model.evaluate(tf_test, steps=total_num_test // batch_size)\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    # 예측 및 F1-score 계산\n",
    "    y_pred = model.predict(pad_x_test)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    print(f\"정확도: {acc:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "34861923",
   "metadata": {},
   "source": [
    "## 결과 분석 \n",
    "\n",
    "![Sample Image](https://i.imgur.com/ZFEB85I.jpeg)\n",
    "\n",
    "\n",
    "\n",
    "전반적으로 vocab_size가 증가할수록 대부분의 머신러닝 모델과 딥러닝의 성능이 향상된다. \n",
    "\n",
    "최적의 vocab_size는 10,000~15,000으로 보이며, 그 이상에서는 성능 향상이 거의 없는 것으로 보인다.\n",
    "\n",
    "\n",
    "(1) Naïve Bayes\n",
    "\n",
    "적은 vocab_size(1,000~3,000)에서도 안정적이지만 성능이 낮음. vocab_size 증가에 따른 성능 향상이 크지 않음\n",
    "각 단어가 개별적으로 작용한다고 가정하는 단순한 모델\n",
    "\n",
    "(2) Complement Naïve Bayes\n",
    "\n",
    "일반 NB보다 성능이 뛰어나며, vocab_size 증가에 따라 성능이 꾸준히 증가됨\n",
    "각 클래스에서 단어 출현 빈도를 보정했기때문에 성능이 더 뛰어남\n",
    "\n",
    "(3) Logistic Regression\n",
    "\n",
    "vocab_size 증가에 따라 큰 성능 향상이 보임. \n",
    "더 많은 단어(feature)를 사용할 수 있어 학습이 더 정밀해진듯\n",
    "\n",
    "(4) Support Vector Classifier \n",
    "\n",
    "vocab_size 증가에 따라 큰 성능 향상이 보임.\n",
    "역시다 더 많은 단어를 사용할 수 있어 학습이 정밀해짐\n",
    "\n",
    "(5) Decision Tree \n",
    "\n",
    "vocab_size 증가에 따라 성능이 약간 감소. \n",
    "단어(feature)가 많아지면서 트리가 너무 복잡해지고, 불필요한 분할이 많아짐 \n",
    "차원의 저주때문 \n",
    "\n",
    "(6) Random Forest \n",
    "\n",
    "성능이 일정 수준 이상으로 유지되며, 10,000 vocab에서 최고 성능을 보임\n",
    "일정 이상(15,000~20,000)에서는 더 이상 정보 이득이 크지 않아서 성능이 정체되는듯\n",
    "\n",
    "(7) Gradient Boosting Tree\n",
    "\n",
    "트리 기반 모델 중 가장 좋은 성능을 보이며, vocab_size 증가에 따라 꾸준한 향상\n",
    "부스팅 기법임\n",
    "\n",
    "\n",
    "(8) Voting\n",
    "\n",
    "가장 높은 성능을 기록하며, vocab_size 증가에 따라 꾸준히 향상됨. 앙상블 학습.\n",
    "\n",
    "\n",
    "(9) Conv1D\n",
    "\n",
    " vocab_size 증가에 따라서 꾸준한 향상, \n",
    "딥러닝 모델은 많은 데이터를 필요로 하며, vocab_size가 증가할수록 성능이 크게 향상됨.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912c6519",
   "metadata": {},
   "source": [
    "## 회고\n",
    "\n",
    "코드들이 너무 돌아가는데 엄청 오래걸려서.. \n",
    "\n",
    "팀원들과 같이 vocab 사이즈를 정하고 해서 모델, vocab 사이즈간 성능비교가 쉬웠음\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
